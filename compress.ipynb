{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dataclasses\n",
    "from typing import Tuple\n",
    "from copy import deepcopy\n",
    "\n",
    "from kws.models import CRNN\n",
    "from test_performance import test_preformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class UncompressedConfig:\n",
    "    keyword: str = 'sheila'  # We will use 1 key word -- 'sheila'\n",
    "    batch_size: int = 128\n",
    "    learning_rate: float = 3e-4\n",
    "    weight_decay: float = 1e-5\n",
    "    num_epochs: int = 20\n",
    "    n_mels: int = 40\n",
    "    cnn_out_channels: int = 8\n",
    "    kernel_size: Tuple[int, int] = (5, 20)\n",
    "    stride: Tuple[int, int] = (2, 8)\n",
    "    hidden_size: int = 32\n",
    "    gru_num_layers: int = 2\n",
    "    bidirectional: bool = False\n",
    "    num_classes: int = 2\n",
    "    sample_rate: int = 16000\n",
    "    device: torch.device = torch.device(\n",
    "        'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    max_window_length: int = 16000\n",
    "    streaming_step_size: int = 1600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:22,  4.54it/s]\n"
     ]
    }
   ],
   "source": [
    "base_config = UncompressedConfig()\n",
    "\n",
    "baseline = test_preformance(UncompressedConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egoriya/miniconda/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "102it [00:20,  4.86it/s]\n"
     ]
    }
   ],
   "source": [
    "reduced_gru = deepcopy(base_config)\n",
    "reduced_gru.gru_num_layers = 1\n",
    "\n",
    "reduced_gru_dict = test_preformance(reduced_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:21,  4.82it/s]\n"
     ]
    }
   ],
   "source": [
    "reduced_channels = deepcopy(base_config)\n",
    "reduced_channels.cnn_out_channels = 4\n",
    "\n",
    "reduced_channels_dict = test_preformance(reduced_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:20,  5.03it/s]\n"
     ]
    }
   ],
   "source": [
    "conf = deepcopy(base_config)\n",
    "conf.hidden_size = 8\n",
    "\n",
    "reduced_hidden_dict = test_preformance(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:20,  4.92it/s]\n"
     ]
    }
   ],
   "source": [
    "conf = deepcopy(base_config)\n",
    "conf.n_mels = 20\n",
    "\n",
    "reduced_mels_dict = test_preformance(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:20,  4.98it/s]\n"
     ]
    }
   ],
   "source": [
    "model = CRNN(base_config)\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, dtype=torch.float16)\n",
    "\n",
    "quantized_dict = test_preformance(base_config, quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:22,  4.60it/s]\n"
     ]
    }
   ],
   "source": [
    "conf = deepcopy(base_config)\n",
    "conf.hidden_size = 8\n",
    "conf.cnn_out_channels = 4\n",
    "conf.n_mels = 20\n",
    "model = CRNN(conf)\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, dtype=torch.float16)\n",
    "\n",
    "reduced_all_dict = test_preformance(conf, model=quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:21,  4.66it/s]\n"
     ]
    }
   ],
   "source": [
    "conf = deepcopy(base_config)\n",
    "conf.hidden_size = 8\n",
    "conf.cnn_out_channels = 4\n",
    "conf.n_mels = 20\n",
    "\n",
    "reduced_noq_dict = test_preformance(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egoriya/miniconda/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "102it [00:21,  4.66it/s]\n"
     ]
    }
   ],
   "source": [
    "conf = deepcopy(base_config)\n",
    "conf.hidden_size = 8\n",
    "conf.cnn_out_channels = 4\n",
    "conf.n_mels = 20\n",
    "conf.gru_num_layers = 1\n",
    "\n",
    "reduced_noq_dict = test_preformance(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: reduced_gru\n",
      "memory reduced by 1.322877\n",
      "au_fa_fr reduced by 1.017830\n",
      "time reduced by 1.954158\n",
      "MACs reduced by 1.200160\n",
      "num_params reduced by 1.332581\n",
      "###\n",
      "###\n",
      "Experiment name: reduced_channels\n",
      "memory reduced by 1.369009\n",
      "au_fa_fr reduced by 0.900967\n",
      "time reduced by 1.410639\n",
      "MACs reduced by 1.559516\n",
      "num_params reduced by 1.404848\n",
      "###\n",
      "###\n",
      "Experiment name: reduced_hidden\n",
      "memory reduced by 4.022875\n",
      "au_fa_fr reduced by 1.039945\n",
      "time reduced by 2.587531\n",
      "MACs reduced by 2.101900\n",
      "num_params reduced by 5.042105\n",
      "###\n",
      "###\n",
      "Experiment name: reduced_mels\n",
      "memory reduced by 1.394914\n",
      "au_fa_fr reduced by 1.040343\n",
      "time reduced by 1.742663\n",
      "MACs reduced by 1.662895\n",
      "num_params reduced by 1.433727\n",
      "###\n",
      "###\n",
      "Experiment name: quantized\n",
      "memory reduced by 0.923332\n",
      "au_fa_fr reduced by 1.647724\n",
      "time reduced by 1.216756\n",
      "MACs reduced by 2.731515\n",
      "num_params reduced by 31.419554\n",
      "###\n",
      "###\n",
      "Experiment name: reduced_all\n",
      "memory reduced by 7.421449\n",
      "au_fa_fr reduced by 0.757687\n",
      "time reduced by 1.939248\n",
      "MACs reduced by 8.151319\n",
      "num_params reduced by 13.065878\n",
      "###\n",
      "###\n",
      "Experiment name: reduced_noq\n",
      "memory reduced by 9.182449\n",
      "au_fa_fr reduced by 0.973573\n",
      "time reduced by 3.669390\n",
      "MACs reduced by 9.068410\n",
      "num_params reduced by 16.801456\n",
      "###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "comp_dict = dict(\n",
    "    reduced_gru = reduced_gru_dict,\n",
    "    reduced_channels = reduced_channels_dict,\n",
    "    reduced_hidden = reduced_hidden_dict,\n",
    "    reduced_mels = reduced_mels_dict,\n",
    "    quantized = quantized_dict,\n",
    "    reduced_all = reduced_all_dict,\n",
    "    reduced_noq = reduced_noq_dict,\n",
    ")\n",
    "\n",
    "for name, cmp in comp_dict.items():\n",
    "    print(f'Experiment name: {name}')\n",
    "    for key in baseline:\n",
    "        print('{} reduced by {:.6f}'.format(key, baseline[key] / cmp[key]))\n",
    "    print('###\\n###')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert observers\n",
    "torch.quantization.prepare(, inplace=True)\n",
    "# Calibrate the model and collect statistics\n",
    "\n",
    "# convert to quantized version\n",
    "torch.quantization.convert(myModel, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a3a81d80e20b06e0334a30c84a66b7a0dd095b5feb0750ed32f27b2a24930b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
